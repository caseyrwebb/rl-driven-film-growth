{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVD Reactor Environment Analysis\n",
    "\n",
    "This notebook analyzes the `CVDReactorEnv` and DQN training results for a basic Arrhenius based CVD environment and RL Training with DQN. We’ll evaluate performance, visualize trajectories, and experiment with hyperparameters to optimize the agent and build intuition for CVD dynamics.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Compare random actions vs. DQN performance (thickness error).\n",
    "- Visualize temperature (T) and flow rate (F) effects on deposition.\n",
    "- Test DQN hyperparameters for better control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cvd_env.reactor_env import CVDReactorEnv\n",
    "from agents.dqn_trainer import train_dqn, evaluate_dqn\n",
    "from utils.plots import plot_trajectories\n",
    "from utils.logger import setup_logging, get_logger\n",
    "\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Random Actions (Baseline)\n",
    "\n",
    "Let’s run the environment with random actions to establish a baseline, as done in `reactor_env.py`’s `__main__` block. This simulates a CVD process without optimization, mimicking manual control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CVDReactorEnv(target_thickness=100.0, max_steps=50)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "random_errors = []\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(int(action))\n",
    "    random_errors.append(info[\"thickness_error\"])\n",
    "    env.render()\n",
    "    done = terminated or truncated\n",
    "\n",
    "logger.info(\n",
    "    \"Random Policy - Final thickness: %.2f nm, Error: %.2f nm\",\n",
    "    obs[0],\n",
    "    info[\"thickness_error\"],\n",
    ")\n",
    "\n",
    "env.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and Evaluate DQN\n",
    "\n",
    "Train a DQN agent for 5000 timesteps (default) and evaluate its performance, as in `dqn_trainer.py`. This shows how RL optimizes the CVD process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CVDReactorEnv(target_thickness=100.0, max_steps=50)\n",
    "model = train_dqn(env, total_timesteps=5000, learning_rate=1e-3)\n",
    "\n",
    "evaluate_dqn(model, env, n_episodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Random vs. DQN\n",
    "\n",
    "Let’s quantify DQN’s improvement by comparing thickness errors across multiple runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "random_errors = []\n",
    "for _ in range(n_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, _, terminated, truncated, info = env.step(int(action))\n",
    "        if terminated or truncated:\n",
    "            random_errors.append(info[\"thickness_error\"])\n",
    "            break\n",
    "\n",
    "\n",
    "dqn_errors = []\n",
    "for _ in range(n_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, terminated, truncated, info = env.step(int(action))\n",
    "        if terminated or truncated:\n",
    "            dqn_errors.append(info[\"thickness_error\"])\n",
    "            break\n",
    "\n",
    "\n",
    "logger.info(\n",
    "    \"Random Policy - Mean Error: %.2f nm, Std: %.2f nm\",\n",
    "    np.mean(random_errors),\n",
    "    np.std(random_errors),\n",
    ")\n",
    "logger.info(\n",
    "    \"DQN Policy - Mean Error: %.2f nm, Std: %.2f nm\",\n",
    "    np.mean(dqn_errors),\n",
    "    np.std(dqn_errors),\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot([random_errors, dqn_errors], labels=[\"Random\", \"DQN\"])\n",
    "plt.ylabel(\"Thickness Error (nm)\")\n",
    "plt.title(\"Random vs. DQN Policy Error Comparison\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Arrhenius Dynamics\n",
    "\n",
    "Analyze how temperature (T) and flow rate (F) affect the deposition rate $$ r = k_0 \\cdot F^{0.5} \\cdot \\exp(-E_a / (R \\cdot T)) $$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deposition rate function\n",
    "def get_deposition_rate(T, F, k0=1e5, Ea=50000, R=8.314, alpha=0.5):\n",
    "    return k0 * (F**alpha) * np.exp(-Ea / (R * T))\n",
    "\n",
    "\n",
    "# Analyze rate vs. T and F\n",
    "T_values = np.linspace(500, 1000, 100)\n",
    "F_values = np.linspace(10, 100, 100)\n",
    "T_grid, F_grid = np.meshgrid(T_values, F_values)\n",
    "rates = get_deposition_rate(T_grid, F_grid)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(T_grid, F_grid, rates, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Deposition Rate (nm/s)\")\n",
    "plt.xlabel(\"Temperature (K)\")\n",
    "plt.ylabel(\"Flow Rate (sccm)\")\n",
    "plt.title(\"Deposition Rate as a Function of T and F\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment with Hyperparameters\n",
    "\n",
    "Test different `total_timesteps` and `learning_rate` to optimize DQN performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_list = [5000, 10000, 20000]\n",
    "errors_by_timesteps = []\n",
    "\n",
    "for timesteps in timesteps_list:\n",
    "    model = train_dqn(env, total_timesteps=timesteps, learning_rate=1e-3)\n",
    "    errors = []\n",
    "    for _ in range(5):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, _, terminated, truncated, info = env.step(int(action))\n",
    "            if terminated or truncated:\n",
    "                errors.append(info[\"thickness_error\"])\n",
    "                break\n",
    "    errors_by_timesteps.append(errors)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(errors_by_timesteps, labels=[str(t) for t in timesteps_list])\n",
    "plt.xlabel(\"Total Timesteps\")\n",
    "plt.ylabel(\"Thickness Error (nm)\")\n",
    "plt.title(\"DQN Performance vs. Training Timesteps\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "- **Random vs. DQN**: DQN should show lower mean error and variance compared to random actions, demonstrating RL’s ability to optimize CVD control.\n",
    "- **Arrhenius Insights**: The heatmap shows T’s exponential impact on deposition rate, guiding control strategies.\n",
    "- **Hyperparameter Tuning**: More timesteps may reduce errors but increase training time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL Driven Film Growth",
   "language": "python",
   "name": "rl-driven-film-growth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
